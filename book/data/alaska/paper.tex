\lefthead{Schleicher}
\righthead{Alaska}
\footer{Data Processing}

\title{A tutorial for processing a 2D land line using Seismic Unix}                                           % Activate to display a given date or no date
\author{Karl Schleicher}
\email{k\_schleicher@hotmail.com}

\maketitle

\begin{abstract}
This paper describes how to process a public 2D land line data set
though a very basic processing sequence using Seismic Unix. The data
from the Alaska North Slope has good signal, although it may be
suitable for testing ground-roll attenuation programs.  The detailed
steps to download the data from the Internet and process it are
described.  You should be able to follow these steps and recreate my
results.  I hope this will accelerate the testing and validation of
programs developed in research groups.
\par
The tutorial starts with background information like data downloading,
region overview, data acquisition, and previous processing.  The
scripts and custom programs to translate to seismic unix, load
headers, qc, apply preprocessing (including velocity filtering and
decon), cdp gather, velocity analysis, and stack can be downloaded
onto your computer.

Header loading is not described in other Seismic Unix documentation.
One approach to header loading is described here.  The velocity
analysis script is better than I found in previous publications.

This paper is the beginning of an effort to build a library of
open-access seismic data sets and processing scripts using open-source
geophysical software. Send me email for the latest results on this line,
and progress on other software and data.
\end{abstract}

\section{Introduction}
Seismic Unix (SU) is a popular open-source seismic processing 
software package \cite[]{TLE16-07-10451049} \cite[]{cohen}.  The 
distribution includes some example processing
scripts and there are two books that teach how to use the software,
\emph{Geophysical Image Processing with Seismic Unix} \cite[]{stockwell} 
and \emph{Seismic Processing with Seismic Un*x} \cite[]{forel}.  These 
reference cover broad subjects including basic Unix commands, setting 
up a user environment, basic seismic processing, and more advanced 
scripting for seismic processing.  You can follow the text and recreate 
some seismic processing.  These books do not provide much information 
about creating trace headers or preprocessing.\nocite{cohen}

I assume you are familiar with Seismic Unix.  I concentrate
on providing example scripts and programs to process a 2D land line
using the processing system. I describe the data acquisition and
previous data processing.  I provide the custom programs I used to
load the trace headers.  The scripts for preprocessing, velocity
analysis, and stacking are provided and explained.  These scripts
provide the detailed parameters used by the Seismic Unix programs.
The parameters for deconvolution, velocity filter, mute, moveout
velocity, and scaling can adapted for use by other seismic processing
systems. The results are similar to the results obtained in 1981 using
GSI's proprietary software.  

You can download Seismic Unix, the field data, and the scripts, and 
recreate my results.  The scripts, data, and programs can be modified 
to validate your own seismic processing ideas.  This should accelerate 
the testing and validation of new seismic research.

\section{Background information about the data set}

President Warren Harding created the Naval Petroleum Reserve Number 4
in 1923.  It was renamed the National Petroleum Reserve, Alaska (NPRA)
in 1976.  Figure~\ref{fig:mapnpra} is a map of the region and 
Figure~\ref{fig:mapusgs} maps the lines collected and processed between 1974
and 1981.  I selected and processed line 31-81 because it was a short
line from the last acquisition season.
\plot{mapnpra}{width=\textwidth}{Map of the National Petroleum
Reserve in Alaska from the \href{http://en.wikipedia.org/wiki/National_Petroleum_Reserve\%E2\%80\%93Alaska}{Wikipedia entry about the National Petroleum
Reserve Alaska}.}
\plot{mapusgs}{width=\textwidth}{Map of seismic lines collects between
1974 and 1981 from the Internet site \url{http://energy.usgs.gov/GeochemistryGeophysics/SeismicDataProcessingInterpretation/NPRASeismicDataArchive.aspx}}

\section{Setting up and reading the data documentation}
\inputdir{line31-81}
\href{http://www.ahay.org/wiki/Download}{Download Madagascar} and you should have files to process line 31-81 in 
the directory:
\$RSFSRC/book/data/alaska/line31-81

Change into this directory.  This paper lists commands and the SConstruct 
file contains the rules that translate the commands into SU tesks 
to process the data.

An alternative to installing Madagascar is to go to the website \url{http://rsf.svn.sourceforge.net/viewvc/rsf/trunk/book/data/alaska/?view=tar}

This will download a tar file that you can install.  On my system I
do this with the commands:
\begin{verbatim}
cd
mv Downloads/rsf-alaska.tar.gz .
tar xvzf rsf-alaska.tar.gz
\end{verbatim}

This will untar the directory.  When it completes, continue:\\
cd open\_lib\_demo\_2011\\
cat readme

This will tell you about the files you downloaded.  You can look at the 
su commands the process will use by typing:\\
cat doit.job

Then you can run the processing by typing:\\  
./doit.job

You can also run the individual tasks with the scripts in the tar file. 
For example, to view the previous final stack, the text says run:\\
scons prevstack.view \\

You can run the script from the tar file:\\
./prevstack.job

The scons will download the file 31\_81\_IM.JPG from the Internet and 
display it using open office draw (oodraw).  The tar file supplies the 
files so the scripts skip the download.  ./prevstack.job just displays
the file using oodraw.

There used to be a higher resolution image of the stack display, the 
file S609.TIF.  This file is no longer available on the USGS Internet 
site.  I was able to view this file in OpenOffice.org draw 
and zoom to read the side label that describes the acquisition and 
processing parameters.

The data was acquired 96 trace, 12 fold with a dynamite source.  The
shotpoint interval is 440 ft and the receiver interval is 110 ft.  The
average shot depth is 75 ft.  The elevation plot on the section header
shows the elevation is about 200 ft and there is little variation.

The processing applied by GSI in 1981 was:
\begin{enumerate}
\item Edit and true amplitude recovery
\item Spherical divergence and exponentiation
(alpha - 4.5 db/s, initial time - 0 s, final time - 4.5 s)
\item Velocity filtering dip range -12 to 4 ms/trace
\item Designature deconvolution
\item Time-variant scaling (unity)
\item Apply datum statics
\item Velocity estimation
\item Normal moveout correction
\item Automatic Residual Statics application
\item First Break Suppression \\
100 ms at 55 ft\\
380 ms at 2970 ft\\
700 ms at 5225 ft
\item Common depth point stack
\item Time Variant filtering \\
16-50 Hz at 700 ms\\
14-45 Hz at 1400 ms\\
12-40 Hz at 2000 ms\\
8-40 Hz at 400 ms
\item Time variant scaling
\end{enumerate}

The surveyor and observer logs are downloaded and displayed with Adobe 
Acrobat (acroread) using the commands:
\\
scons surveylog.view \\
scons observerlog.view

I read the surveyor's log and typed the elevations into the spnElev.txt 
file.  spnElev.txt contains two columns.  The first is the shotpoint 
number and the second is the elevation.  I read the observer's log and 
typed the relationship between the field record numbers the shotpoint 
numbers into the recnoSpn.txt file.  recnoSpn.txt contains two columns.  
The first column is the field record number and the second column is the 
shotpoint number. I used a shotpoint number -999999 to reject bad field 
records.  These files will be used in the next section to load the trace 
headers.

\section{Data Loading and initial data QC}

I found little documentation about loading data and creating trace
headers in previous publications.  This section provides a detailed
description.

The three field data segy files are downloaded, converted to Seismic 
Unix format, concatenated, and a print created of the range of each trace 
header using the command\\

scons list1.su

The resulting listing is:
6868 traces:\\
tracl    1 6868 (1 - 6868)\\
tracr    1 6868 (1 - 6868)\\
fldr     101 168 (101 - 168)\\
tracf    1 101 (1 - 101)\\
trid     1\\
ns       3000\\
dt       2000\\
f1       0.000000 512.000000 (0.000000 - 512.000000)

The print indicates there is little information in the input headers.
There are 31 shot records (fldr 101 - 130) and each record has 101
traces (tracf 1 - 101).  The trace headers will be created using fldr
(the field record number) and tracf (the field trace number).  A list 
these headers on the first 3000 traces is created using:

scons list2.su 

The first part of the print is: \\
fldr=101 tracf=1  cdp=0	 cdpt=0  offset=0 \\
fldr=101 tracf=2  cdp=0	 cdpt=0  offset=0 \\	
fldr=101 tracf=3  cdp=0  cdpt=0  offset=0 \\
... \\
fldr=101 tracf=101  cdp=0  cdpt=0  offset=0 \\
fldr=102 tracf=1    cdp=0  cdpt=0  offset=0 \\
fldr=102 tracf=2    cdp=0  cdpt=0  offset=0 \\
...

A first display of the seismic data and a zoom of the same data 
(figures~\ref{fig:first} and ~\ref{fig:zoomfirst}) are
produced with the commands: \\
scons first.view \\
scons zoomfirst.view

\plot{first}{width=\textwidth}{First 3000 traces on the first segy file.}
\plot{zoomfirst}{width=\textwidth}{Zoom of Figure~\ref{fig:first}. The 
first look at the data. Notice the ground roll and good signal. There 
are 101 traces in each shotpoint, 5 auxiliary traces and 96 data traces.}

The first 10 records in the file are test records.  These test
records are the unusual traces in the left third of
Figure~\ref{fig:first}. The Observer Log does not mention these test
records, but indicates the first shotpoint is field file identifier 
(ffid) 110.  There are also 5 auxiliary traces in each shot record.  
These auxiliary traces can be seen on Figure~\ref{fig:zoomfirst}. 

The display of shotpoint 24 (Figure ~\ref{fig:zoomfirst}) is displayed 
with the command: \\
scons firstrec24.view

This display as created using suwind to select traces 2324 through 2424, 
so it is the 24th record in the input file (including the test records). 
This plot confirms that the 5 auxiliary traces are tracf 97 through 101.

\plot{firstrec24}{width=\textwidth}{The 24th shot record without 
processing.  The last 5 traces traces are auxiliary traces.  There is 
noise on the traces closest to the shotpoint.  One trace has a noise burst 
near the maximum time.}

The movie display is not included in this paper.  The movie loops 
continuously and can be stopped by pressing 's'.  Once stopped, you 
can move forward or backward using the 'f' and 'b' keys.  The movie can be 
run with the command:\\
scons firstmovie.view


The trace headers are loaded using the command:\\
scons allshots.su

The stages in loading the trace header are
\begin{enumerate}
\item sugethw to create the file hdrfile.txt containing one record for 
each trace with tracl, fldr, tracf
\item run the custom python program InterpText.py.  This program:\\
reads recnoSpn.txt that defines the recno/spn relationship
reads spnElev.txt  the defined the elevation of each shot
interpolates this data and output hdrfile1.txt with values for all keys
to be loaded
\item converts hdrfile1.txt to binary format (binary\_hdrfile1.dat)
\item loads the trace headers
\item removes bad shotpoints and cdps (with value=-999999)
\item removes the bad shotpoint 149
\item outputs allshots.su
\end{enumerate}

InterpText.py is a custom python program to load the trace headers.  The
code is not elegant, but it is straightforward and works.  This program 
require files to define the shotpoint elevation and the 'ffid'/'ep' 
relationship.  The surveyor log defines the shotpoint elevations.  The 
observers log describes the relationship between the su header keys 
'ffid' and 'ep' (these are called 'REC' and 'SHOTPOINT' by in the observers 
log).  This information was typed into two files (recnoSpn.txt and 
spnElev.txt).  In addition to interpolating the record number/shotpoint 
table and the elevation data the program also computes the geometry 
including the receiver locations and the CDP numbers.

The stack file from the previous process is downloaded and displayed 
using the commands: \\
scons checkstack.view \\
scons zoomcheckstack.view

The first result is shown in Figure~\ref{fig:checkstack}.  The zoom display
is not shown.

\plot{checkstack}{width=\textwidth}{Plot of the segy of the final stack 
from the 1981 processing.}

\section{Shot record velocity filter}
I applied Velocity filtering (sudipfilt) to remove the groundroll.
I processed the receivers leading the shotpoint separately from the
trailing receivers (i.e. I split the shots).  I plit the shots so I 
could use an asymmetrical dip filter (-15,5 ms/trace).  That dip filter 
is similar to the 1981 processing that used (-12,4). Sudipfilter was 
intended for post stack processing, so I wrote the script alaskavelfilt.sh 
to loop over the shots and divide individual shotrecords into positive 
and negative offsets.  Figure~\ref{fig:velfiltrec24} is the same shotpoint 
as plotted in Figure~\ref{fig:zoomfirst} with velocity filtering applied.

The command:\\
scons velfiltrec24.view \\
applies velocity filterring and produces the figure.

\plot{velfiltrec24}{width=\textwidth}{The 24th shot record after 
agc and velocity filter.}

\section{cdp sort and mute}

The command\\
scons cdp250-251.view \\
sorts the data to CDP order, applies a mute and display cdps 250 and 
251.  This result is shown in Figure~\ref{fig:cdp250-251}.  I picked 
the mute from the stacked section.  The first trace on the stack is 
created from a single far offset which, of course, has moveout applied.  
In order to recreate this mute I applied moveout using a single velocity 
function contained in the file vbrureorig.txt. After applying the mute, 
I reversed the moveout.
\plot{cdp250-251}{width=\textwidth}{CDP gathers 250 and 251 with mute applied}

\section{Velocity analysis and residual statics strategy}
A problem encountered in residual statics estimation is the coupling of 
velocity and statics.  Changing the medium wavelength statics (.5 to 3 
spread length) will change the medium wavelength stacking velocity.  
The classic approach to this problem is to repeat the velocity analysis 
and residual static estimation sequence.  Incorporating rapid lateral 
variation in the initial stacking velocity will prevent the medium wavelength 
estimation by residual statics.  The undesirable rapid velocity variation 
is locked into the final velocity field.  My strategy is 
commonly used.  I ran velocity analysis and statics twice.  On the first 
iteration, I used single function velocity function for moveout before 
residual static estimation.  If the line was longer I might have run
a coarsely sampled set of velocity analyzes. After the first 
pass of residual statics,  I ran velocity analysis every 48 CDPs (2640 ft 
or 5. spreadlength).  I did not notice improvement when I compared the 
stack with the first pass of residual statics and the velocity field from 
the 48 CDP velocity analysis with the results after two passes of residual 
statics.  The next sections step through the velocity analysis and residual 
static processes.

\section{First Pass Velocity analysis}
I used a long script, iva.sh, that combines several su programs for velocity 
interpretation.  My iva.sh is a combination of the scripts iva.sh and 
velanQC.sh in sections 7.6.7.3 and 8.2.2.2 of 
\emph{Seismic Processing with Seismic Un*x} \cite[]{forel}.  My script is 
more practical, because you can start from no velocity function, or review 
and update an existing velocity field.  The script has more capabilities, 
is tidier, and is a little shorter than Forel's script.

Figure~\ref{fig:velanal} is an example velocity analysis plot.  It can 
be produced with the commands: \\
rm vbrute.txt \\
scons vbrute.txt

There are four components of the plot, semblance plot, constant velocity 
stack (CVS) plot, cmp gather without NMO and cmp gather after NMO.  The 
semblance plots and CVS plot have the velocity function over plotted.  
The velocity analysis can be picked by pointing on the semblance and 
'selecting' by pressing the 's' key.  To 'quit' the location press the 
'q' key.  If you have 'selected' any picks, the plots will be recreated 
with your new velocity function, otherwise processing will continue at 
the next location.  (The brute analysis only has one velocity location, 
so 'q' just quits the velocity analysis.)

The velocities I picked are in the vbrute.txt file.

\inputdir{.}
\plot{velanal}{width=\textwidth}{Example velocity analysis from iva.sh}
\inputdir{line31-81}

\section{Brute Stack}
The brute stack can be produced with the command: \\
scons brutestack.view

This command reads the cdp gather data and applies decon using supef.  In 
order to define a decon design gate that depends on offset, I used sustatic 
to time shift the data to line up where I wanted the time gate to start.  
After decon (supef) the time shift is removed.  Moveout using the brute 
velocity is applied followed by agc and a mute.  This data is written to a 
file which is reread to create the brute stack shown in 
Figure~\ref{fig:brutestack}.  You can create a zoom of this plot using the 
command:\\
scons zoombrutestack.view

You can make a movie of the gathers with moveout applied with the command: \\
scons movie\_velfiltcdpsnmo.view

\plot{brutestack}{width=\textwidth}{Brute stack.}

\section{First Pass Residual Statics and Second Pass Velocity Analysis}
The command:\\
scons rstack.view\\
Will create a stack with residual statics and a detailed velocity analysis 
by the following steps:
\begin{enumerate}
\item Read the gathers created in the brute stack section
\item Bandpass filter, select a time window, and up sample to 1 ms
\item Compute residual statics
\item Reread the gathers created in the brute stack section
\item Apply the residual statics
\item Remove moveout with the single velocity function 
\item Second pass velocity estimation every 48 cdps (2640 ft)
\item Normal moveout correction with the second pass velocity field 
\item Stack
\end{enumerate}

The residual statics stack is shown in Figure~\ref{fig:rstack}.  A zoom 
plot can be produce with the command:\\
scons zoomrstack.view

\plot{rstack}{width=\textwidth}{Stack with first pass residual statics and 
a lateral variable stacking velocity field.}

The lateral variable stacking velocity field shown in Figure~\ref{fig:vfile} 
can be created with the command:\\
scons vfile.view

\plot{vfile}{width=\textwidth}{The lateral variable stacking velocity 
field interpreted after residual statics.}

\section{Second Pass Residual Statics}
The command:\\
scons rstack1.view\\
creates a stack with second pass residuals shown in Figure~\ref{fig:rstack1}.  
I compared figures~\ref{fig:rstack} and~\ref{fig:rstack1} and did not see 
any improvement.\\
\\  
A zoom plot can be produce with the command:\\
scons zoomrstack1.view

I enjoyed comparing the zoom plots of the initial brute stack and the stack 
with two iterations of residual statics and velocity analysis.  These plots
are not included in the paper, but you can create them with the commands:\\
scons zoombrutestack.view\& \\
scons zoomrstack1.view\& \\
Make the plots full screen size and toggle between the windows using the 
buttons on the bottom run bar.  The event disruptions that line up vertically
are static problems solved by the residual statics process.
  
\plot{rstack1}{width=\textwidth}{The final stack with the lateral variable 
stacking velocity and a second pass of residual statics.}

\section{post stack migration}
The commands:\\
scons mig.view\\
scons migps.view\\
apply post stack to the second pass residual statics stack 
(Figure~\ref{fig:rstack1}). The first command produces the Kirchhoff 
migration shown Figure~\ref{fig:mig} and the second command produces the 
phase shift migration shown in Figure~\ref{fig:migps}.  The Kirchhoff data 
used the final stacking velocity field (Figure~\ref{fig:vfile}).  The phase 
shift migration used a single interval velocity.  I converted to interval 
velocity using Dix' equation in a spreadsheet

\plot{mig}{width=\textwidth}{Post stack Kirchhoff migration of the 
final stack.}
\plot{migps}{width=\textwidth}{Post stack Kirchhoff migration of the 
final stack.}

\section{Comparison of the SU and the 1981 results}
Figures~\ref{fig:checkstack} and~\ref{fig:rstack1} are the results created 
obtained in 1981 and the results obtained using SU.  Some of the differences 
in the processing sequences are:
\begin{enumerate}
\item The 1981 processing used GSI's designature process.  This source 
signature removal technique applies the same operator to all traces in a 
shotrecord.  The filter is designed differently from conventional 
deconvolution and the two types of deconvolution will not produce data 
with the same phase. 
\item There are many differences in the application of AGC.  For example 
GSI applied AGC before velocity filtering and inverse AGC after velocity 
filter.
\item GSI used diversity stack to control the high amplitude noise bursts.
\end{enumerate}
Considering all these differences, I think the results are surprisingly 
similar, especially below 400 ms.  The shallow result are better on the 
1981 processing.

\section{Discussion of the Seismic Unix processing}
I found the SU software hard to use.  Sudipfilter was not intended for 
shotrecord velocity filtering, so a collection of program were required.  
Many of my input errors were not trapped by the code so I struggled to 
debug my scripts.  SU is a useful prototyping environment, but it falls 
short of the commercial packages I have used.

\section{Plans}
Some of the ideas I have to continue this work include:

\begin{enumerate}
\item Process using other software (Open source or proprietary). 
\item Study difference and see if the open source software can be improved.
\item Working on different datasets (marine, 3D, ...).
\end{enumerate}

\section{Conclusions}
I have provided a set of scripts the process a land line using SU.  These scripts include the processing stages:

\begin{enumerate}
\item Data load
\item Trace header creation
\item Velocity filtering 
\item CDP gather
\item Brute velocity analysis
\item First pass residual statics
\item Final velocity analysis
\item Second pass residual statics
\item Stack
\item Post stack kirchhoff time migration
\item Post stack phase shift migration
\end{enumerate}

My processing illustrates processing stages not presented by Forel
or Stockwell. The results can be recreated by others since the processing
scripts and the data can be downloaded from the Internet.

The task of loading trace headers is not addressed in other
publications.  The custom python program included in this paper is one
way to complete this processing stage.  The velocity analysis script 
improves the scripts in Forel's book.  I was able to pick the velocities 
on this line using the script.

Some of the ideas I have to continue this work include:
\begin{enumerate}
\item Improve results by selecting different SU programs or different 
parameters. 
\item Process using other software (Open source or proprietary)
\item Working on more data (Marine, 3D, ...)
\item Contribute source improvements to SU.
\end{enumerate}

\bibliographystyle{seg}
\bibliography{refpaper,SEG}


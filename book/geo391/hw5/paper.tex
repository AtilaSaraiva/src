\author{Thomas Bayes}
%%%%%%%%%%%%%%%%%%%%
\title{Homework 5 \\ Due on Monday, December 13}

\begin{abstract}
  This homework has five parts. 
  \begin{enumerate}
  \item Theoretical questions related to linear estimation.
  \item Attenuation of surface-wave noise in land 
    seismic data using match filtering.
  \item Missing data interpolation for ocean floor topography. 
  \item Missing data interpolation for ocean floor topography using data patterns. 
  \item Spatial interpolation contest using rainfall data from Switzerland.
  \end{enumerate}
\end{abstract}

\section{Linear estimation}

\begin{enumerate}
\item The following algorithm finds a solution to
  the least-squares optimization problem $\min
  \|\mathbf{F}\,\mathbf{m} - \mathbf{d}\|^2$, where $\mathbf{d}$ is
  data, $\mathbf{m}$ is the model we want to estimate, and
  $\mathbf{F}$ is a linear operator that connects them.

 \begin{algorithm}{Conjugate gradients}{\mathbf{F},\mathbf{d},N}
  \mathbf{m \= 0} \\
  \mathbf{r \= - d} \\
  \begin{FOR}{n \= 1, 2, \ldots, N} \\
    \mathbf{g}_m \= \mathbf{F}^T\,\mathbf{r} \\
    \mathbf{g}_r \= \mathbf{F}\,\mathbf{g}_m \\
    \rho \= \mathbf{g}_m^T\,\mathbf{g}_m \\
    \begin{IF}{n = 1} 
      \beta \= 0 
      \ELSE 
      \beta \= \rho/\hat{\rho} 
    \end{IF} \\
    \left[\begin{array}{l}
        \mathbf{s}_m \\
        \mathbf{s}_r
      \end{array}\right] \= 
    \left[\begin{array}{l}
        \mathbf{g}_m \\
        \mathbf{g}_r
      \end{array}\right] + \beta\,
    \left[\begin{array}{l}
        \mathbf{s}_m \\
        \mathbf{s}_r
      \end{array}\right] \\
    \alpha \= - \rho/(\mathbf{s}_r^T\,\mathbf{s}_r) \\
    \left[\begin{array}{l}
        \mathbf{m} \\
        \mathbf{r}
      \end{array}\right] \= 
    \left[\begin{array}{l}
        \mathbf{m} \\
        \mathbf{r}
      \end{array}\right] + \alpha\,
    \left[\begin{array}{l}
        \mathbf{s}_m \\
        \mathbf{s}_r
      \end{array}\right] \\
    \hat{\rho} \= \rho
  \end{FOR} \\        
  \RETURN \mathbf{m}
\end{algorithm}
\begin{enumerate}
\item  In applications, it is often advantageous to apply \emph{model
    reparameterization} or \emph{preconditioning}. Suppose that,
  instead of solving for $\mathbf{m}$ directly, you first solve for
  $\mathbf{x}$ such that $\mathbf{m} = \mathbf{P}\,\mathbf{x}$. Show
  how to incorporate the linear preconditioning operator $\mathbf{P}$
  in the algorithm above.
\item Prove that the output of the algorithm after $N$ iterations is
\begin{equation}
\label{eq:msol}
\mathbf{m}_N = \sum\limits_{n=1}^{N} \frac{\mathbf{s}_n\,\mathbf{s}_n^T}{\mathbf{s}_n^T\,\mathbf{F}^T\,\mathbf{F}\,\mathbf{s}_n}\,\mathbf{F}^T\,\mathbf{d}\;,
\end{equation}
where $\mathbf{s}_n$ is the model step at $n$-th iteration.
\item Assuming that $\mathbf{m}$ is the true model, show that
\begin{equation}
\label{eq:mres}
\mathbf{m}_N = \sum\limits_{n=1}^{N} \frac{\mathbf{g}_n\,\mathbf{g}_n^T}{\mathbf{g}_n^T\,\mathbf{g}_n}\,\mathbf{m}\;,
\end{equation}
where $\mathbf{g}_n$ is the gradient at $n$-th iteration.
\end{enumerate}

\item Assuming the model $\mathbf{m}$ has the probability distribution function 
\begin{equation}
\label{eq:pm}
P(\mathbf{m}) = \frac{1}{\left(\pi\,\det[\mathbf{C}_m]\right)^{N/2}}\,e^{-(\mathbf{m}-\mathbf{m}_0)^T\,\mathbf{C}_m^{-1}\,(\mathbf{m}-\mathbf{m}_0)}\;,
\end{equation}
where $\mathbf{C}_m$ is the model covariance matrix,
and the conditional probability of data $\mathbf{d}$ (given model $\mathbf{m}$) is
\begin{equation}
\label{eq:pdm}
P(\mathbf{d}|\mathbf{m}) = \frac{1}{\left(\pi\,\det[\mathbf{C}_n]\right)^{N/2}}\\,e^{-(\mathbf{d}-\mathbf{F}\,\mathbf{m})^T\,\mathbf{C}_n^{-1}\,(\mathbf{d}-\mathbf{F}\,\mathbf{m})}\;,
\end{equation}
where $\mathbf{C}_n$ is the noise covariance matrix, Bayesian inversion suggests the model estimate (given data $\mathbf{d}$) of the form
\begin{equation}
\label{eq:mest}
\mathbf{m}_{*} = \mathbf{m}_0 + 
\mathbf{C}_m\,\mathbf{F}^T\,\left(\mathbf{F}\,\mathbf{C}_m\,\mathbf{F}^T + \mathbf{C}_n\right)^{-1}\,\left(\mathbf{d} - \mathbf{F}\,\mathbf{m}_0\right)
\end{equation} 
\begin{enumerate}
\item Find the \emph{bias} in estimate~(\ref{eq:mest}), i.e. for the true model $\mathbf{m}$, find the mathematical expectation of $\mathbf{m}_{*}-\mathbf{m}$.
\item Shaping regularization suggests an estimate
\begin{equation}
\label{eq:shape}
\mathbf{m}_{*} = \mathbf{m}_0 + \left[\mathbf{I} + \mathbf{S}\,(\mathbf{B}\,\mathbf{F}-\mathbf{I})\right]^{-1}\,\mathbf{S}\,\mathbf{B}\,\left(\mathbf{d} - \mathbf{F}\,\mathbf{m}_0\right)\;,
\end{equation}
where $\mathbf{B}$ is the ``backward'' operator, $\mathbf{S}$ is the ``model shaping'' operator, and $\mathbf{I}$ is the identity operator.  Find the connection between $\mathbf{F}$, $\mathbf{B}$,  $\mathbf{S}$, $\mathbf{C}_m$, and $\mathbf{C}_n$ that makes equations~(\ref{eq:mest}) and~(\ref{eq:shape})
equivalent.
\end{enumerate}
\end{enumerate} 

\section{Match Filtering for Attenuation of Surface Seismic Waves}
\inputdir{match}

\sideplot{data}{width=\textwidth}{Seismic shot record from sand dunes in the Middle East. The data are contaminated by ground roll propagating in the sand.}

Figure~\ref{fig:data} shows a section out of a seismic shot record
collected over sand dunes in the Middle East. The data are
contaminated by ground roll propagating in the sand. A major data
analysis task is to separate the signal (reflection waves) from the
noise (surface waves).

\plot{spec0}{width=0.8\textwidth}{Data spectrum. Solid line -- original data. Dashed line -- initial noise model and signal model.}

A look at the data spectrum (Figure~\ref{fig:spec0} shows that the
noise is mostly concentrated at low frequencies. We can use this fact
to create a noise model by low-pass filtering.

\plot{noise0}{width=\textwidth}{(a) Noise model created by low-pass filtering of the original data. (b) Result of subtraction of the noise model from the data.}

Figure~\ref{fig:noise0} shows the noise model from low-pass filtering
and inner muting and the result of subtracting this model from the
data. Our next task is to match the model to the true noise by solving
the least-squares optimization problem
\begin{equation}
\label{eq:ls}
\min \|\mathbf{N}_0\,\mathbf{f} - \mathbf{d}\|^2\;,
\end{equation}
where $d$ is the data, $f$ is a \emph{matching filter}, and
$\mathbf{N}_0$ represents convolution of the noise model
$\mathbf{n}_0$ with the filter. After minimization, $\mathbf{n} =
\mathbf{N}_0\,\mathbf{f}$ becomes the new noise model, and
$\mathbf{d}-\mathbf{n}$ becomes the estimated signal. Match filtering
is implemented in program \texttt{match.c}. Some parts of this program
are left out for you to fill.

\lstset{language=c,numbers=left,numberstyle=\tiny,showstringspaces=false}
\lstinputlisting[frame=single,firstline=19]{match/match.c}

Your task:
\begin{enumerate}
\item Change directory to \verb#geo391/hw5/match#
\item Run 
\begin{verbatim}
scons view
\end{verbatim}
to reproduce the figures on your screen.
\item Modify the \texttt{match.c} file to fill in missing parts.
\item Test your modifications by running the dot product test.
\begin{verbatim}
scons dot.test
\end{verbatim}
Repeating this several times, make sure that the numbers in the test match.
\item  Modify the \texttt{SConstruct} file to display the results of match filtering
and include them in your assignment.
\item \textbf{EXTRA CREDIT} for improving the results by either finding better parameters or by finding a better algorithm. 
\end{enumerate}

\lstset{language=python,numbers=left,numberstyle=\tiny,showstringspaces=false}
\lstinputlisting[frame=single]{match/SConstruct}

\section{Missing ocean floor data interpolation, part 1}
\inputdir{seabeam}

SeaBeam is an apparatus for measuring water depth both directly under
a boat and somewhat off to the sides of the boat's track. In this part
of the assignment, we will use a benchmark dataset from \cite{gee}:
SeaBeam data from a single day of acquisition. The original data are
shown in Figure~\ref{fig:mesh}a.

\plot{mesh}{width=\textwidth}{(a) Water depth measurements from one day
  of SeaBeam acquisition. (b) Random initial model with covariance specified by the inverse Laplacian filter.}

To find missing data, we will use estimate~(\ref{eq:mest}), where
$\mathbf{F}$ is the mask operator (a diagonal matrix with ones and
zeros on the diagonal using ones to mask the known data locations),
$\mathbf{C}_m$ is a stationary filter, and $\mathbf{C}_n$ is close to
zero,

Our first choice for $\mathbf{C}_m$ is the inverse of a five-point
Laplacian filter
\begin{equation}
\label{eq:lap2}
L_2(Z_1,Z_2) = 4 - Z_1 - 1/Z_1 - Z_2 - 1/Z_2
\end{equation} 
To build the inverse, we put the Laplacian
filter on a helix, where it takes the form
\begin{equation}
\label{eq:lap1}
L_1(Z) = 4 - Z - 1/Z - Z^{N_1} - 1/Z^{N_1}
\end{equation} 
and factor it into two parts $L_1(Z) = D(Z)\,D(1/Z)$ using the
Wilson-Burg algorithm \cite[]{burg}. The factorization is tested in
Figure~\ref{fig:laplace}, where the impulse response of the Laplacian
filter gets inverted by recursive filtering (polynomial division) on a
helix. 

\plot{laplace}{width=\textwidth}{Impulse response of the five-point Laplacian filter (a) gets inverted by recursive filtering (polynomial division) on a helix. 
(b) Division by $D(Z)$. (c) Division by $D(1/Z)$. (d) Division by $D(Z)\,D(1/Z)$.}

The Laplacian filter is a common a priori choice for enforcing
smoothness in the estimated model. Figure~\ref{fig:mesh}b shows a
random model created by dividing random normally distributed noise by
$D(Z)$. We will use it as an initial model for the missing data
reconstruction problem. 

There are two alternative formulations of Bayesian least-squares inversion:
\begin{description}
\item[Overdetermined least-squares] Minimize the power (least-squares length) of the data misfit $\widehat{\mathbf{d}} - \widehat{\mathbf{F}}_m\,\mathbf{m}$, where
\begin{equation}
\label{eq:dhat}
\widehat{\mathbf{d}} = \left[\begin{array}{l} \mathbf{D_n}\,\left(\mathbf{d} - \mathbf{F}\,\mathbf{m}_0\right) \\
\mathbf{0}\,\end{array}\right]\;,
\end{equation} 
\begin{equation}
\label{eq:fmhat}
\widehat{\mathbf{F}}_m = \left[\begin{array}{l} \mathbf{D_n}\,\mathbf{F} \\
\mathbf{D_m}\,\end{array}\right]\;,
\end{equation} 
with $\mathbf{C}_m^{-1} = \mathbf{D_m}^T\,\mathbf{D_m}$ and $\mathbf{C}_n^{-1} =
\mathbf{D_n}^T\,\mathbf{D_n}$. In our case, $\mathbf{D_n}$ is multiplication
by a large number, and $\mathbf{D_m}$ is convolution with $D(Z)$ (polynomial \emph{multiplication} on a helix).
\item[Underdetermined least-squares] Minimize the power (least-squares length) of the extended model $\widehat{\mathbf{m}}$ provided 
that $\mathbf{d} = \mathbf{F}\,\mathbf{m}_0 + \widehat{\mathbf{F}}_d\,\widehat{\mathbf{m}}$, where
\begin{equation}
\label{eq:mhat}
\widehat{\mathbf{m}} = \left[\begin{array}{l} \mathbf{p} \\
\mathbf{q}\,\end{array}\right]\;,
\end{equation} 
\begin{equation}
\label{eq:fdhat}
\widehat{\mathbf{F}}_d = \left[\begin{array}{cc} \mathbf{F}\,\mathbf{P_m} &
\mathbf{P_n}\,\end{array}\right]\;
\end{equation}
with $\mathbf{m} = \mathbf{P_m}\,\mathbf{p}$, $\mathbf{C}_m = \mathbf{P_m}^T\,\mathbf{P_m}$ and $\mathbf{C}_n =
\mathbf{P_n}^T\,\mathbf{P_n}$. In our case, $\mathbf{P_n}$ is multiplication
by a small number, and $\mathbf{P_m}$ is convolution with $1/D(Z)$ (polynomial \emph{division} on a helix).
\end{description}

The results of missing data reconstruction from both methods after 10 conjugate-gradient iterations are shown in Figure~\ref{fig:interp}.

\plot{interp}{width=\textwidth}{Result of missing data interpolation after 10 iterations using (a) polynomial multiplication and (b) polynomial division on a helix.}

%\newpage
Your task:
\begin{enumerate}
\item Change directory to \verb#geo391/hw5/seabeam#
\item Run 
\begin{verbatim}
scons view
\end{verbatim}
to reproduce the figures on your screen.
\item Modifying the \texttt{SConstruct} file to accomplish the following tasks
\begin{enumerate}
\item Find out the number of conjugate-gradient iterations, when the ``multiplication'' and ``division'' methods produce visually similar results.
\item Find out which of the two methods converges to this solution faster.
\item Replace the five-point Laplacian filter with the more isotropic nine-point filter 
\begin{eqnarray}
\nonumber
\hat{L}_2(Z_1,Z_2) = 20 & - & 4\,Z_1 - 4/Z_1 - 4\,Z_2 - 4/Z_2 \\
& - & Z_1\,Z_2 - Z_1/Z_2 - Z_2/Z_1 - 1/(Z_1\,Z_2)
\label{eq:lap9}
\end{eqnarray}
and repeat the experiment.
\end{enumerate}
\end{enumerate}

\lstset{language=python,numbers=left,numberstyle=\tiny,showstringspaces=false}
\lstinputlisting[frame=single]{seabeam/SConstruct}

\section{Missing ocean floor data interpolation, part 2}
\inputdir{seabeam2}

In this section, we return to the problem of interpolating ocean floor
data, discussed in Homework 5. Program~\texttt{interpolate.c}
implements two alternative methods: regularized inversion (similar to
equation~\ref{eq:laplace} but using a multi-dimensional PEF for
$\mathbf{R}$) and preconditioning, which uses the inverse of
$\mathbf{R}$ (recursive deconvolution or polynomial division on a
helix) for model reparameterization.

\plot{data}{width=0.8\textwidth}{Left: input data. Right: mask for known data values.}

The input data and a mask for known values are shown in
Figure~\ref{fig:data}. Results from the two methods are shown in Figure~\ref{fig:seabeam}.

\plot{seabeam}{width=0.8\textwidth}{Left: missing data interpolation using regularization by convolution with a prediction-error filter. 
Right: missing data interpolation using model reparameterization by deconvolution (polynomial division) with a prediction-error filter.}

\lstset{language=c,numbers=left,numberstyle=\tiny,showstringspaces=false}
\lstinputlisting[frame=single,firstline=19]{seabeam2/interpolate.c}

\lstset{language=python,numbers=left,numberstyle=\tiny,showstringspaces=false}
\lstinputlisting[frame=single]{seabeam2/SConstruct}

Your task:
\begin{enumerate}
\item Change directory to \verb#geo391/hw5/seabeam2#
\item Run 
\begin{verbatim}
scons view
\end{verbatim}
to reproduce the figures on your screen.
\item Modify the \texttt{SConstruct} file to implement the following tasks
\begin{enumerate}
\item Find the number of iterations required for both methods to achieve similar results.
\item Generate multiple interpolation realizations using the method of the previous section.
\item To provide a more quantitative comparison, modify the
\texttt{interpolate.c} program to output a measure of convergence
(such as the least-squares model misfit) as a function of the number
of iterations\footnote{You can study the interfaces to
the \texttt{sf\_solver} and
\texttt{sf\_solver\_prec} programs. See
\url{http://rsf.svn.sourceforge.net/viewvc/rsf/trunk/api/c/bigsolver.c?view=markup}}. Generate figures comparing convergence with and without preconditioning.
\end{enumerate}
\item Include your results in the paper.
\end{enumerate}

\section{Spatial interpolation contest}
\inputdir{rain}

In 1997, the European Communities organized a Spatial Interpolation
Comparison. Many different organizations participated with the results
published in a special issue of the \emph{Journal of Geographic
Information and Decision Analysis} \cite[]{dubois} and a separate
report \cite[]{rain}.

\sideplot{elev}{width=\textwidth}{Digital elevation map of Switzerland.}

The comparison used a dataset from rainfall measurements in
Switzerland on the 8th of May 1986, the day of the Chernobyl disaster.
Figure~\ref{fig:elev} shows the data area: the Digital Elevation Model
of Switzerland with superimposed country's borders.  A total of 467
rainfall measurements were taken that day. A randomly selected subset
of 100 measurements was used as the input data the 1997 Spatial
Interpolation Comparison in order to interpolate other measurements
using different techniques and to compare the results with the known
data. Figure~\ref{fig:raindata} shows the spatial locations of the
selected data samples and the full dataset.

\plot{raindata}{width=\textwidth}{Left: locations of weather stations used as input data in the spatial interpolation contest.
Right: all weather stations locations.}

In this assignment, you will try different techniques of spatial data
interpolation and will participate in the interpolation contest.

\subsection{Delaunay triangulation}

The first technique we are going to try is Delaunay triangulation with
linear interpolation of rainfall values inside each triangle. The
result is shown in Figure~\ref{fig:trian}. Does it succeed in hiding
the acquisition footprint? Figure~\ref{fig:trian-pred} provides a
comparison between interpolated and known data values. It also
indicates the value of the correlation coefficient.

\multiplot{2}{trian,trian-pred}{width=0.45\textwidth}{(a) Rainfall data
interpolated using Delaunay triangulation. (b) Correlation between
interpolated and true data values.}

\subsection{Laplacian regularization}

An alternative technique is a solution of the regularized
least-squares optimization problem
\begin{equation}
\label{eq:laplace}
\min\left( \|\mathbf{L}\,\mathbf{m} - \mathbf{d}\|^2 + \epsilon^2 \|\mathbf{R}\,\mathbf{m}\|^2\right)\;,
\end{equation}
where $\mathbf{d}$ is irregular data, $\mathbf{m}$ is model estimated
on a regular grid, $\mathbf{L}$ is forward interpolation from the
regular grid to irregular locations, $\epsilon$ is a scaling
parameter, and $\mathbf{R}$ is the regularization operator related to
the inverse of the assumed model covariance. In this experiment,
$\mathbf{R}$ is the finite-difference approximation of the Laplacian operator.

\plot{laplac}{width=\textwidth}{Rainfall data
interpolated using regularization with the Laplacian filter.}

Figure~\ref{fig:laplac} shows the interpolation result after 10 and
1,000 iterations. Even 1,000 iterations are not enough to converge to
an acceptable solution, as is evident from the correlation analysis in
Figure~\ref{fig:laplac1000-pred}.

\sideplot{laplac1000-pred}{width=\textwidth}{Correlation between
  interpolated and true data values for Laplacian regularization with
  1,000 iterations.}

\subsection{Shaping regularization}

The next approach is shaping regularization: an iterative solution of the inverse problem
\begin{equation}
\widehat{\mathbf{m}} = 
  \left(\mathbf{L}^T\,\mathbf{L} + \mathbf{S}^{-1} -
    \mathbf{I}\right)^{-1}\,\mathbf{L}^T\,\mathbf{d}
  = \left[\mathbf{I} + 
    \mathbf{S}\,\left(\mathbf{L}^T\,\mathbf{L} - \mathbf{I}\right)\right]^{-1}\,
  \mathbf{S}\,\mathbf{L}^T\,\mathbf{d}\;,
  \label{eqn:shape}  
\end{equation}
where $\mathbf{S}$ is the shaping operator, which, in this experiment,
is taken as a two-dimensional triangle smoothing.

\plot{shape}{width=\textwidth}{Rainfall data
interpolated using shaping regularization with a triangle filter.}

Figure~\ref{fig:shape} shows the interpolation result after 10 and the
maximum number of iterations. The correlation analysis with the
ground-truth data is shown in Figure~\ref{fig:shape1000-pred}.

\sideplot{shape1000-pred}{width=\textwidth}{Correlation between
  interpolated and true data values for Shaping regularization.}

\subsection{Your task}

\begin{enumerate}
\item Change directory to \verb#geo391/hw5/rain#
\item Run 
\begin{verbatim}
scons view
\end{verbatim}
to reproduce the figures on your screen.
\item Modify the \texttt{SConstruct} file to find the number of iterations required by the Laplacian regularization to converge.
\item What can you conclude about the three methods used in this comparison?
\item Participate in the Spatial Interpolation Contest. Find and
implement a method that would provide a better interpolation of the
missing values than either of the methods we tried so far. You can change any of the parameters in the existing methods or 
write your own program but you can use only the 100 original data points as input.
\end{enumerate}

\lstset{language=python,numbers=left,numberstyle=\tiny,showstringspaces=false}
\lstinputlisting[frame=single]{rain/SConstruct}

\newpage

\section{Completing the assignment}

\begin{enumerate}
\item Change directory to \verb#geo391/hw5#.
\item Edit the file \texttt{paper.tex} in your favorite editor and change the
  first line to have your name instead of Bayes's.
\item Run
\begin{verbatim}
sftour scons lock
sftour scons -c
\end{verbatim}
and
\begin{verbatim}
scons pdf
\end{verbatim}
\item Submit your result (file \texttt{paper.pdf}) by printing it out
  or by e-mail.
\end{enumerate}

\bibliographystyle{seg}
\bibliography{bayes}

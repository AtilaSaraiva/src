from rsf.proj import *

########################################################################
####################### TRIP DEMO SCONSTRUCT ###########################
########################################################################

###### JOB DICTIONARY: PYTHON DICTIONARY OF JOB NAME AND CORRESPONDING 
###### COMMAND, DEPENDENCY, AND OTHER NECESSARY EXECUTION INFO
######
###### FORM:
######   jobs = {
######     <job name> : [<command string>, (other stuff - see below)],
######     ...
######   }
######
###### ASSUMPTIONS:
###### - JOB NAME IS ALSO ROOT NAME OF PAR FILE IN THIS DIRECTORY
######   example: job name = 'data', then par file name = './data.par'
######   (exception: tmpdata task, see below)
###### - JOB NAME WILL BE USED AS NAME OF BUILD SUBDIRECTORY FOR JOB
######   example: job name = 'data', then script builds subdir ./data
######   and executes job in this directory
###### - EACH JOB EXECUTES EXACTLY ONE COMMAND
###### - COMMANDS ARE ASSUMED TO BE BUILT - BUILD COMMANDS FIRST!
###### - COMMAND NAME IS COMPLETE, WITH ALL NECESSARY SCRIPTS, FLAGS, 
######   AND OPTIONS
######   example: to use mpirun on 8 processors to execute cmd.x which
######   resides in PATH_TO_COMMANDS (environment), the command string 
######   should read 
######   'mpirun -np 8 $PATH_TO_COMMANDS/cmd.x'
###### - JOB BUILD COMMAND: 'jobs[jobname] par=jobname.par'
######   example: if jobs dictionary includes this line:
######   'data' : 'mpirun -np 256 $IWAVE/esg/main/esg.x'
######   then the command to build this job is
######   'mpirun -np 256 $IWAVE/esg/main/esg.x par=data.par'
######   and this string is fed to a shell (interactive) or embedded in
######   script which is then submitted (batch)
###### - SCONS CLEAN COMMAND SHOULD REMOVE ALL JOB DIRECTORIES CREATED
######   BY THIS SCRIPT (IT WILL!), AND THESE CONTAIN ALL OUTPUT OF THE
######   CORRESPONDING COMMANDS. CONSEQUENCE: NO FILE CAN BE BOTH INPUT
######   AND OUTPUT FOR THE SAME TARGET.
######   example: a SEGY file cannot be both the source of header info 
######   and the output data file - so a feature of early IWAVE commands
######   cannot be exercised in these demos
###### - Intermediate data is by convention stored in $DATAPATH/iwave, which
######   is created by target tmpdata. Therefore this target should be always
######   be included in the scripts dictionary, unless no intermediate data at all 
######   is involved in any task (unlikely!!!)

###### Structure of job dictionary:
###### key    = name of job = name of output directory
###### 
###### A dictionary specifies each job. These keys are standard:
###### 'cmd'  = path to command (may be empty only if a special branch of make takes care of this target)
###### 'deps' = dependency list (may be empty)
###### 'exec' = parallel/batch key 
######          possible values:
######            '' (serial)
######            'mpi' (command-line parallel with mpirun)
######            'pbs' (batch submission via PBS)
######            'sge' (batch submission via SGE)
###### 'wc'   = wallclock limit (hr:min:sec)
###### 'nodes'= nodes
###### 'ppn'  = processes per node
###### 'queue'= queue name
###### 'email'= email for notification
###### 'acct' = account name 
######
###### For fetch commands:
###### 'subd' = subdirectory of server data directory in which to search
###### 'url'  = URL of server

###### convenient strings literals for these:

CMD   = 'cmd'
EXEC  = 'exec'
DEPS  = 'deps'
WC    = 'wc'
NODES = 'nodes'
PPN   = 'ppn'
MAIL  = 'mail'
ACCT  = 'acct'
Q     = 'queue'
DATA  = 'subd'
URL   = 'url'

import os

################### ENVIRONMENT-DEPENDENT USER INPUT ###################

########################################################################
######################### JOB DEFINITION TABLE #########################
########################################################################

### commands
IWAVE         = os.getenv('IWAVE')
asg           = os.path.join(IWAVE,'asg/main/asg.x')
mdl           = os.path.join(IWAVE,'demo/main/standardmodel.x')

### jobs driven by parameter tables:
jobs = {
'dn3d_40m': {CMD : mdl, DEPS : ['d.tmpdata'] },
'vp3d_40m': {CMD : mdl, DEPS : ['d.tmpdata'] },
'dome3d40m' : {CMD : asg, DEPS : ['d.sudata', 'd.dn3d_40m', 'd.vp3d_40m'] },
'dome3d40mp' : {CMD : asg, DEPS : ['d.sudata', 'd.dn3d_40m', 'd.vp3d_40m'], EXEC : 'mpi', NODES : 1, PPN : 2 },
'dome3d40m_pt2' : {CMD : asg, DEPS : ['d.sudata', 'd.dn3d_40m', 'd.vp3d_40m'], EXEC : 'mpi', NODES : 1, PPN : 2 },
'dome3d40m_pt4' : {CMD : asg, DEPS : ['d.sudata', 'd.dn3d_40m', 'd.vp3d_40m'], EXEC : 'mpi', NODES : 1, PPN : 4 },
}

### scripts:
scripts = {
'tmpdata' : { DEPS : None },
'sudata'  : { DEPS : ['d.tmpdata'] },
}

### remote data fetches:
fetches = {
}

########################################################################
# generally the right choice for temp data - can be modified ###########
########################################################################

DATAPATH = os.getenv('DATAPATH','.')
TMPDATAPATH = os.path.join(DATAPATH,'iwave')

############### END ENVIRONMENT-DEPENDENT USER INPUT ###################

########################################################################
###### interface to sconscript files - do not edit below this line #####
########################################################################

# register jobs, scripts, and fetch targets as targets
for tgt in jobs.keys():
    Default('d.'+tgt)

for tgt in scripts.keys():
    Default('d.'+tgt)

for file in fetches.keys():
    tgt = TMPDATAPATH + '/' + file
    Default(tgt)

# all other tasks are assumed to be dependencies of the fetches
#if len(fetches.keys()) > 0 :
    from rsf.proj import *
    for file in fetches.keys():
        tgt = TMPDATAPATH + '/' + file
        Fetch(tgt,fetches[file][DATA],server=fetches[file][URL])   
	for job in jobs.keys():
	    Depends('d.'+job,tgt)
        for script in scripts.keys():
            Depends('d.'+script,tgt)

# necessary if any scripts use SU, MPI
CWPPATH = os.getenv('CWPROOT','.')
MPIPATH = os.getenv('MPIROOT','.')
print 'CWPPATH='+CWPPATH+' MPIPATH='+MPIPATH

for tgt in jobs.keys():
    prep = '/bin/rm -rf d.' + tgt + '; /bin/mkdir d.' + tgt + '; cd d.' + tgt + '; cat ../' + tgt + '.par| sed s:"{TMPDATAPATH}":' + TMPDATAPATH + ': > parfile; ' 
    if EXEC in jobs[tgt].keys():
        if jobs[tgt][EXEC] == '':
            make = jobs[tgt][CMD] + ' par=parfile';
        if jobs[tgt][EXEC] == 'mpi':
	    MPIRUN = os.path.join(MPIPATH,'bin/mpirun')
	    print 'MPIRUN='+MPIRUN
            make = MPIRUN + ' -np ' + str(int(jobs[tgt][NODES]) * int(jobs[tgt][PPN])) + ' ' + jobs[tgt][CMD] + ' par=./parfile' 
    else:
        make = jobs[tgt][CMD] + ' par=parfile';	
    t=Flow('d.'+ tgt, jobs[tgt][DEPS], prep + make, stdin=0, stdout=0)
    Clean(t,'d.'+tgt)
      
for tgt in scripts.keys():
    make = 'CWPROOT=' + CWPPATH + '; export CWPROOT; DATAPATH=' + TMPDATAPATH + '/; export DATAPATH; /bin/rm -rf d.' + tgt + '; /bin/mkdir d.' + tgt + '; . ./' + tgt + '.sc'
    print tgt + ' = ' + make
    t=Flow('./d.' + tgt, scripts[tgt][DEPS], make, stdin=0, stdout=0)
    if tgt == 'tmpdata':
        Clean(t,['d.'+tgt, TMPDATAPATH])
    else:
        Clean(t,'d.'+tgt)

End()